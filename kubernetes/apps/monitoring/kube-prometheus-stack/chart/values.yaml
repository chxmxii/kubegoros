namespaceOverride: monitoring

cleanPrometheusOperatorObjectNames: true

alertmanager:
  enabled: true

  alertmanagerSpec:
    replicas: 2
    useExistingSecret: true
    configSecret: alertmanager-config

  # ingress:
  #   enabled: true

  #   ingressClassName: internal

  #   annotations:
  #     kubernetes.io/tls-acme: "true"
  #     cert-manager.io/cluster-issuer: letsencrypt-prod
  #     nginx.ingress.kubernetes.io/auth-url: https://auth.plexmox.com/api/verify
  #     nginx.ingress.kubernetes.io/auth-signin: https://auth.plexmox.com

  #   hosts:
  #     - alerts.plexmox.com

  #   tls:
  #     - secretName: alerts-plexmox-com-tls
  #       hosts:
  #         - alerts.plexmox.com

nodeExporter:
  enabled: true

kubelet:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      # Drop high cardinality labels
      - action: labeldrop
        regex: (uid)
      - action: labeldrop
        regex: (id|name)
      - action: drop
        sourceLabels: ["__name__"]
        regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)
      - action: drop
        sourceLabels: ["__name__"]
        regex: (apiserver|etcd)_request(|_sli|_slo)_duration_seconds_bucket
      - action: drop
        sourceLabels: ["__name__"]
        regex: apiserver_request_body_size_bytes_bucket
      - action: drop
        sourceLabels: ["__name__"]
        regex: apiserver_(response|watch_events)_sizes_bucket
      - action: drop
        sourceLabels: ["__name__"]
        regex: (kubelet_runtime_operations|storage_operation)_duration_seconds_bucket
      - action: drop
        sourceLabels: ["__name__"]
        regex: (apiserver|etcd)_request_total
      - action: drop
        sourceLabels: ["__name__"]
        regex: (etcd|apiserver)_request_duration_seconds_(count|sum)
    probesMetricRelabelings:
      - action: drop
        sourceLabels: ["__name__"]
        regex: prober_probe_duration_seconds_bucket
    cAdvisorMetricRelabelings:
      - action: labeldrop
        regex: (uid)
      - action: labeldrop
        regex: (id|name)
      - action: labeldrop
        regex: (endpoint|instance)

kubeApiServer:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      # Drop high cardinality labels
      - action: drop
        sourceLabels: ["__name__"]
        regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
      - action: drop
        sourceLabels: ["__name__"]
        regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)
      - action: drop
        sourceLabels: ["__name__"]
        regex: apiserver_request_body_size_bytes_bucket
      - action: drop
        sourceLabels: ["__name__"]
        regex: workqueue_(queue|work)_duration_seconds_bucket
      - action: drop
        sourceLabels: ["__name__"]
        regex: apiserver_admission_controller_admission_duration_seconds_bucket

kubeEtcd:
  enabled: true
  endpoints:
    - 10.254.100.10
    - 10.254.100.11
    - 10.254.100.15
  serviceMonitor:
    metricRelabelings:
      - action: drop
        sourceLabels: ["__name__"]
        regex: (etcd_request_duration_seconds_bucket)

kubeControllerMnager:
  enabled: true
  endpoints:
    - 10.254.100.10
    - 10.254.100.11
    - 10.254.100.15

kubeProxy:
  enabled: true
  endpoints:
    - 10.254.100.10
    - 10.254.100.11
    - 10.254.100.15

kubeScheduler:
  enabled: true
  endpoints:
    - 10.254.100.10
    - 10.254.100.11
    - 10.254.100.15

defaultRules:
  rules:
    kubeControllerManager: false
    kubeProxy: false
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: false

grafana:
  enabled: true
  replicas: 1

  # admin:
  #   existingSecret: grafana-creds

  # env:
  #   GF_AUTH_GENERIC_OAUTH_API_URL: https://auth.plexmox.com/api/oidc/userinfo
  #   GF_AUTH_GENERIC_OAUTH_AUTH_URL: https://auth.plexmox.com/api/oidc/authorization
  #   GF_AUTH_GENERIC_OAUTH_CLIENT_ID: grafana
  #   GF_AUTH_GENERIC_OAUTH_TOKEN_URL: https://auth.plexmox.com/api/oidc/token
  #   GF_DATABASE_NAME: grafana
  #   GF_DATABASE_HOST: mariadb.mariadb.svc:3306
  #   GF_DATABASE_USER: grafana
  #   GF_DATABASE_SSL_MODE: disable
  #   GF_DATABASE_TYPE: mysql

  # envValueFrom:
  #   GF_DATABASE_PASSWORD:
  #     secretKeyRef:
  #       name: grafana-creds
  #       key: db_password
  #   GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:
  #     secretKeyRef:
  #       name: grafana-creds
  #       key: client_secret

  # grafana.ini:
  #   server:
  #     root_url: https://grafana.plexmox.com
  #   feature_toggles:
  #     enable: metricsSummary tempoServiceGraph traceToMetrics
  #   auth:
  #     oauth_auto_login: true
  #   auth.basic:
  #     enabled: true
  #   auth.generic_oauth:
  #     enabled: true
  #     name: Authelia
  #     icon: signin
  #     scopes: openid profile email groups
  #     empty_scopes: false
  #     login_attribute_path: preferred_username
  #     groups_attribute_path: groups
  #     name_attribute_path: name
  #     use_pkce: true
  #   auth.generic_oauth.group_mapping:
  #     org_id: 1
  #     role_attribute_path: |
  #       contains(groups[*], 'admin') && 'Admin' || contains(groups[*], 'people') && 'Viewer'
  ingress:  
    enabled: true

    ingressClassName: nginx

    ## Annotations for Grafana Ingress
    ##
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"

    ## Labels to be added to the Ingress
    ##
    # labels: {}

    ## Hostnames.
    ## Must be provided if Ingress is enable.
    ##
    hosts:
      - grafana.k8s.coralio.co
    # hosts: []

    ## Path for grafana ingress
    path: /

    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    ##
    tls:
    - secretName: grafana-tls
      hosts:
      - grafana.k8s.coralio.co

  #   ingressClassName: internal

  #   annotations:
  #     kubernetes.io/tls-acme: "true"
  #     cert-manager.io/cluster-issuer: letsencrypt-prod
  #     nginx.ingress.kubernetes.io/auth-url: https://auth.plexmox.com/api/verify
  #     nginx.ingress.kubernetes.io/auth-signin: https://auth.plexmox.com

  #   hosts:
  #     - grafana.plexmox.com

  #   tls:
  #     - secretName: grafana-plexmox-com-tls
  #       hosts:
  #         - grafana.plexmox.com

  # defaultDashboardsEditable: false

  # sidecar:
  #   dashboards:
  #     annotations:
  #       grafana_folder: Kubernetes
  #     folderAnnotation: grafana_folder
  #     provider:
  #       foldersFromFilesStructure: true
    # datasources:
    #   defaultDatasourceEnabled: false

  # dashboardProviders:
  #   dashboardproviders.yaml:
  #     apiVersion: 1
  #     providers:
  #       - name: default
  #         orgId: 1
  #         folder: Custom
  #         type: file
  #         disableDeletion: true
  #         editable: false
  #         options:
  #           path: /var/lib/grafana/dashboards/default
  #       - name: postgres
  #         orgId: 1
  #         folder: Postgres
  #         type: file
  #         disableDeletion: true
  #         editable: false
  #         options:
  #           path: /var/lib/grafana/dashboards/postgres


  additionalDataSources:
    # - name: Prometheus
    #   orgId: 1
    #   access: proxy
    #   url: http://thanos-query-frontend.thanos.svc.cluster.local:10902
    #   jsonData:
    #     prometheusType: Thanos
    #     timeInterval: 1m
    #   isDefault: true
    # - name: Loki
    #   ordId: 1
    #   access: proxy
    #   type: loki
    #   url: http://loki-gateway
    #   jsonData:
    #     maxLines: 250
    # - name: Tempo
    #   orgId: 1
    #   access: proxy
    #   type: tempo
    #   url: http://tempo.monitoring.svc:3100

prometheus:
  ingress:
    enabled: true

    ingressClassName: nginx

    annotations:
      kubernetes.io/tls-acme: "true"
      cert-manager.io/cluster-issuer: letsencrypt
  #     nginx.ingress.kubernetes.io/auth-url: https://auth.plexmox.com/api/verify
  #     nginx.ingress.kubernetes.io/auth-signin: https://auth.plexmox.com

    hosts:
      - prometheus.k8s.coralio.co

    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus.k8s.coralio.co

  # thanosService:
  #   enabled: true
  # thanosServiceMonitor:
  #   enabled: true

  prometheusSpec:
    replicas: 1
    # replicaExternalLabelName: __replica__

    enableRemoteWriteReceiver: true
    scrapeConfigSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false

    # thanos:
    #   objectStorageConfig:
    #     existingSecret:
    #       name: thanos-objstore-config
    #       key: config.yaml

    # externalLabels:
    #   cluster: main

    # retention: 14d
    # retentionSize: 25GB

    # walCompression: true

    # enableFeatures:
    #   - memory-snapshot-on-shutdown
    #   - new-service-discovery-manager

    # storageSpec:
    #   volumeClaimTemplate:
    #     spec:
    #       storageClassName: longhorn-no-backups
    #       accessModes: ["ReadWriteOnce"]
    #       resources:
    #         requests:
    #           storage: 30Gi


# # Default values for kube-prometheus-stack.
# # This is a YAML-formatted file.
# # Declare variables to be passed into your templates.

# ## Provide a name in place of kube-prometheus-stack for `app:` labels
# ##
# nameOverride: ""

# ## Override the deployment namespace
# ##
# namespaceOverride: ""

# ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6
# ##
# kubeTargetVersionOverride: ""

# ## Allow kubeVersion to be overridden while creating the ingress
# ##
# kubeVersionOverride: ""

# ## Provide a name to substitute for the full names of resources
# ##
# fullnameOverride: ""

# ## Labels to apply to all resources
# ##
# commonLabels: {}
# # scmhash: abc123
# # myLabel: aakkmd

# crds:
#   enabled: true

# defaultRules:
#   create: true
#   rules:
#     alertmanager: true
#     etcd: true
#     configReloaders: true
#     general: true
#     k8sContainerCpuUsageSecondsTotal: true
#     k8sContainerMemoryCache: true
#     k8sContainerMemoryRss: true
#     k8sContainerMemorySwap: true
#     k8sContainerResource: true
#     k8sContainerMemoryWorkingSetBytes: true
#     k8sPodOwner: true
#     kubeApiserverAvailability: true
#     kubeApiserverBurnrate: true
#     kubeApiserverHistogram: true
#     kubeApiserverSlos: true
#     kubeControllerManager: true
#     kubelet: true
#     kubeProxy: true
#     kubePrometheusGeneral: true
#     kubePrometheusNodeRecording: true
#     kubernetesApps: true
#     kubernetesResources: true
#     kubernetesStorage: true
#     kubernetesSystem: true
#     kubeSchedulerAlerting: true
#     kubeSchedulerRecording: true
#     kubeStateMetrics: true
#     network: true
#     node: true
#     nodeExporterAlerting: true
#     nodeExporterRecording: true
#     prometheus: true
#     prometheusOperator: true

# alertmanager:

#   ## Deploy alertmanager
#   ##
#   enabled: true

#   ## Annotations for Alertmanager
#   ##
#   annotations: {}

#   ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2
#   ##
#   apiVersion: v2
#   ingress:
#     enabled: false

#     # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
#     # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
#     # ingressClassName: nginx

#     annotations: {}

#     labels: {}

#     ## Override ingress to a different defined port on the service
#     # servicePort: 8081
#     ## Override ingress to a different service then the default, this is useful if you need to
#     ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)
#     # serviceName: kube-prometheus-stack-alertmanager-0

#     ## Hosts must be provided if Ingress is enabled.
#     ##
#     hosts: []
#       # - alertmanager.domain.com

#     ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
#     ##
#     paths: []
#     # - /

#     ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
#     ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
#     # pathType: ImplementationSpecific

#     ## TLS configuration for Alertmanager Ingress
#     ## Secret must be manually created in the namespace
#     ##
#     tls: []
#     # - secretName: alertmanager-general-tls
#     #   hosts:
#     #   - alertmanager.example.com

#   ## Configuration for Alertmanager secret
#   ##
#   secret:
#     annotations: {}

#   ## Configuration for creating an Ingress that will map to each Alertmanager replica service
#   ## alertmanager.servicePerReplica must be enabled
#   ##
#   ingressPerReplica:
#     enabled: false

#     # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
#     # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
#     # ingressClassName: nginx

#     annotations: {}
#     labels: {}

#     ## Final form of the hostname for each per replica ingress is
#     ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
#     ##
#     ## Prefix for the per replica ingress that will have `-$replicaNumber`
#     ## appended to the end
#     hostPrefix: ""
#     ## Domain that will be used for the per replica ingress
#     hostDomain: ""

#     ## Paths to use for ingress rules
#     ##
#     paths: []
#     # - /

#     ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
#     ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
#     # pathType: ImplementationSpecific

#     ## Secret name containing the TLS certificate for alertmanager per replica ingress
#     ## Secret must be manually created in the namespace
#     tlsSecretName: ""

# grafana:
#   enabled: true
#   namespaceOverride: ""

  
#   ingress:
#     ## If true, Grafana Ingress will be created
#     ##
#     enabled: true

#     ## IngressClassName for Grafana Ingress.
#     ## Should be provided if Ingress is enable.
#     ##
#     ingressClassName: nginx

#     ## Annotations for Grafana Ingress
#     ##
#     annotations:
#       kubernetes.io/ingress.class: nginx
#       kubernetes.io/tls-acme: "true"

#     ## Labels to be added to the Ingress
#     ##
#     # labels: {}

#     ## Hostnames.
#     ## Must be provided if Ingress is enable.
#     ##
#     hosts:
#       - grafana-monitoring.k8s.coralio.co
#     # hosts: []

#     ## Path for grafana ingress
#     path: /

#     ## TLS configuration for grafana Ingress
#     ## Secret must be manually created in the namespace
#     ##
#     tls:
#     - secretName: grafana-tls
#       hosts:
#       - grafana-monitoring.k8s.coralio.co

  # # To make Grafana persistent (Using Statefulset)
  # #
  # persistence:
  #   enabled: true
  #   type: sts
  #   storageClassName: "storageClassName"
  #   accessModes:
  #     - ReadWriteOnce
  #   size: 20Gi
  #   finalizers:
  #     - kubernetes.io/pvc-protection
  
# prometheus:

#   enabled: true

#   ## Toggle prometheus into agent mode
#   ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.
#   ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md
#   ##
#   agentMode: false

#   ingress:
#     enabled: true

#     # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
#     # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
#     ingressClassName: nginx

#     annotations: {}
#     labels: {}

#     ## Redirect ingress to an additional defined port on the service
#     # servicePort: 8081

#     ## Hostnames.
#     ## Must be provided if Ingress is enabled.
#     ##
#     hosts:
#       - prometheus.monitoring.k8s.coralio.co
#     # hosts: []

#     ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
#     ##
#     paths: []
#     # - /

#     ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
#     ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
#     # pathType: ImplementationSpecific

#     ## TLS configuration for Prometheus Ingress
#     ## Secret must be manually created in the namespace
#     ##
#     tls: 
#       - secretName: prometheus-tls
#         hosts:
#           - prometheus.monitoring.k8s.coralio.co

#   ## Configuration for creating an Ingress that will map to each Prometheus replica service
#   ## prometheus.servicePerReplica must be enabled
#   ##
  
#     ## Image of Prometheus.
#     ##
#     image:
#       registry: quay.io
#       repository: prometheus/prometheus
#       tag: v2.52.0
#       sha: ""

    
#     resources: {}
#     # requests:
#     #   memory: 400Mi

#     ## Prometheus StorageSpec for persistent data
#     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
#     ##
#     storageSpec: {}
#     ## Using PersistentVolumeClaim
#     ##
#     #  volumeClaimTemplate:
#     #    spec:
#     #      storageClassName: gluster
#     #      accessModes: ["ReadWriteOnce"]
#     #      resources:
#     #        requests:
#     #          storage: 50Gi
#     #    selector: {}

#     # Additional volumes on the output StatefulSet definition.
#     volumes: []

#     # Additional VolumeMounts on the output StatefulSet definition.
#     volumeMounts: []
#     prometheusSpec:
#     serviceMonitorSelector:
#       matchLabels:
#         prometheus: argo-cd-grafana
#     additionalScrapeConfigs: |
#       - job_name: 'argocd'
#         scrape_interval: 30s
#         scrape_timeout: 10s
#         static_configs:
#           - targets: ['argocd-application-controller-metrics.argocd.svc.cluster.local:8082', 'argocd-redis-metrics.argocd.svc.cluster.local:9121']
#       - job_name: 'alertManager'
#         scrape_interval: 30s
#         scrape_timeout: 10s
#         static_configs:
#           - targets: ['alertmanager-operated.argocd.svc.cluster.local:9093']


# kubeEtcd:
#   enabled: true

#   ## If your etcd is not deployed as a pod, specify IPs it can be found on
#   ##
#   endpoints: []
#   # - 10.254.100.11
#   # - 10.254.100.12
#   # - 10.254.100.15

#   ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
#   ##
#   service:
#     enabled: true
#     port: 2381
#     targetPort: 2381
#     ipDualStack:
#       enabled: false
#       ipFamilies: ["IPv6", "IPv4"]
#       ipFamilyPolicy: "PreferDualStack"
#     # selector:
#     #   component: etcd


#     enabled: true
#     port: 2381
#     targetPort: 2381
#     ipDualStack:
#       enabled: false
#       ipFamilies: ["IPv6", "IPv4"]
#       ipFamilyPolicy: "PreferDualStack"
#     # selector:
#     #   component: etcd

#   ## Configure secure access to the etcd cluster by loading a secret into prometheus and
#   ## specifying security configuration below. For example, with a secret named etcd-client-cert
#   ##
#   ## serviceMonitor:
#   ##   scheme: https
#   ##   insecureSkipVerify: false
#   ##   serverName: localhost
#   ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
#   ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
#   ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
#   ##
#   serviceMonitor:
#     enabled: true
#     ## Scrape interval. If not set, the Prometheus default scrape interval is used.
#     ##
#     interval: ""

#     ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
#     ##
#     sampleLimit: 0

#     ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
#     ##
#     targetLimit: 0

#     ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelLimit: 0

#     ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelNameLengthLimit: 0

#     ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelValueLengthLimit: 0

#     ## proxyUrl: URL of a proxy that should be used for scraping.
#     ##
#     proxyUrl: ""
#     scheme: http
#     insecureSkipVerify: false
#     serverName: ""
#     caFile: ""
#     certFile: ""
#     keyFile: ""

#     ## port: Name of the port the metrics will be scraped from
#     ##
#     port: http-metrics

#     jobLabel: jobLabel
#     selector: {}
#     #  matchLabels:
#     #    component: etcd

#     ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
#     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#     ##
#     metricRelabelings: []
#     # - action: keep
#     #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
#     #   sourceLabels: [__name__]

#     ## RelabelConfigs to apply to samples before scraping
#     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#     ##
#     relabelings: []
#     # - sourceLabels: [__meta_kubernetes_pod_node_name]
#     #   separator: ;
#     #   regex: ^(.*)$
#     #   targetLabel: nodename
#     #   replacement: $1
#     #   action: replace

#     ## Additional labels
#     ##
#     additionalLabels: {}
#     #  foo: bar

# ## Component scraping kube scheduler
# ##
# kubeScheduler:
#   enabled: true

#   ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
#   ##
#   endpoints: []
#   # - 10.254.100.11
#   # - 10.254.100.12
#   # - 10.254.100.15

#   ## If using kubeScheduler.endpoints only the port and targetPort are used
#   ##
#   service:
#     enabled: true
#     ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
#     ## of default port in Kubernetes 1.23.
#     ##
#     port: null
#     targetPort: null
#     ipDualStack:
#       enabled: false
#       ipFamilies: ["IPv6", "IPv4"]
#       ipFamilyPolicy: "PreferDualStack"
#     # selector:
#     #   component: kube-scheduler

#   serviceMonitor:
#     enabled: true
#     ## Scrape interval. If not set, the Prometheus default scrape interval is used.
#     ##
#     interval: ""

#     ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
#     ##
#     sampleLimit: 0

#     ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
#     ##
#     targetLimit: 0

#     ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelLimit: 0

#     ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelNameLengthLimit: 0

#     ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelValueLengthLimit: 0

#     ## proxyUrl: URL of a proxy that should be used for scraping.
#     ##
#     proxyUrl: ""
#     ## Enable scraping kube-scheduler over https.
#     ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
#     ## If null or unset, the value is determined dynamically based on target Kubernetes version.
#     ##
#     https: null

#     ## port: Name of the port the metrics will be scraped from
#     ##
#     port: http-metrics

#     jobLabel: jobLabel
#     selector: {}
#     #  matchLabels:
#     #    component: kube-scheduler

#     ## Skip TLS certificate validation when scraping
#     insecureSkipVerify: null

#     ## Name of the server to use when validating TLS certificate
#     serverName: null

#     ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
#     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#     ##
#     metricRelabelings: []
#     # - action: keep
#     #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
#     #   sourceLabels: [__name__]

#     ## RelabelConfigs to apply to samples before scraping
#     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#     ##
#     relabelings: []
#     # - sourceLabels: [__meta_kubernetes_pod_node_name]
#     #   separator: ;
#     #   regex: ^(.*)$
#     #   targetLabel: nodename
#     #   replacement: $1
#     #   action: replace

#     ## Additional labels
#     ##
#     additionalLabels: {}
#     #  foo: bar

# ## Component scraping kube proxy
# ##
# kubeProxy:
#   enabled: true

#   ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
#   ##
#   endpoints: []
#   # - 10.141.4.22
#   # - 10.141.4.23
#   # - 10.141.4.24

#   service:
#     enabled: true
#     port: 10249
#     targetPort: 10249
#     ipDualStack:
#       enabled: false
#       ipFamilies: ["IPv6", "IPv4"]
#       ipFamilyPolicy: "PreferDualStack"
#     # selector:
#     #   k8s-app: kube-proxy

#   serviceMonitor:
#     enabled: true
#     ## Scrape interval. If not set, the Prometheus default scrape interval is used.
#     ##
#     interval: ""

#     ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
#     ##
#     sampleLimit: 0

#     ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
#     ##
#     targetLimit: 0

#     ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelLimit: 0

#     ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelNameLengthLimit: 0

#     ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#     ##
#     labelValueLengthLimit: 0

#     ## proxyUrl: URL of a proxy that should be used for scraping.
#     ##
#     proxyUrl: ""

#     ## port: Name of the port the metrics will be scraped from
#     ##
#     port: http-metrics

#     jobLabel: jobLabel
#     selector: {}
#     #  matchLabels:
#     #    k8s-app: kube-proxy

#     ## Enable scraping kube-proxy over https.
#     ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
#     ##
#     https: false

#     ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
#     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#     ##
#     metricRelabelings: []
#     # - action: keep
#     #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
#     #   sourceLabels: [__name__]

#     ## RelabelConfigs to apply to samples before scraping
#     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#     ##
#     relabelings: []
#     # - action: keep
#     #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
#     #   sourceLabels: [__name__]

#     ## Additional labels
#     ##
#     additionalLabels: {}
#     #  foo: bar

# ## Component scraping kube state metrics
# ##
# kubeStateMetrics:
#   enabled: true

# ## Configuration for kube-state-metrics subchart
# ##
# kube-state-metrics:
#   namespaceOverride: ""
#   rbac:
#     create: true
#   releaseLabel: true
#   prometheus:
#     monitor:
#       enabled: true

#       ## Scrape interval. If not set, the Prometheus default scrape interval is used.
#       ##
#       interval: ""

#       ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
#       ##
#       sampleLimit: 0

#       ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
#       ##
#       targetLimit: 0

#       ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#       ##
#       labelLimit: 0

#       ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#       ##
#       labelNameLengthLimit: 0

#       ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
#       ##
#       labelValueLengthLimit: 0

#       ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
#       ##
#       scrapeTimeout: ""

#       ## proxyUrl: URL of a proxy that should be used for scraping.
#       ##
#       proxyUrl: ""

#       # Keep labels from scraped data, overriding server-side labels
#       ##
#       honorLabels: true

#       ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
#       ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#       ##
#       metricRelabelings: []
#       # - action: keep
#       #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
#       #   sourceLabels: [__name__]

#       ## RelabelConfigs to apply to samples before scraping
#       ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
#       ##
#       relabelings: []
#       # - sourceLabels: [__meta_kubernetes_pod_node_name]
#       #   separator: ;
#       #   regex: ^(.*)$
#       #   targetLabel: nodename
#       #   replacement: $1
#       #   action: replace

#   selfMonitor:
#     enabled: false

# ## Deploy node exporter as a daemonset to all nodes
# ##
# nodeExporter:
#   enabled: true
#   operatingSystems:
#     linux:
#       enabled: true
#     darwin:
#       enabled: true

#   ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
#   ##
#   forceDeployDashboards: false

# ## Configuration for prometheus-node-exporter subchart
# ##
# # prometheus-node-exporter:
# #   namespaceOverride: ""
# #   podLabels:
# #     ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
# #     ##
# #     jobLabel: node-exporter
# #   releaseLabel: true
# #   extraArgs:
# #     - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
# #     - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
# #   service:
# #     portName: http-metrics
# #     ipDualStack:
# #       enabled: false
# #       ipFamilies: ["IPv6", "IPv4"]
# #       ipFamilyPolicy: "PreferDualStack"